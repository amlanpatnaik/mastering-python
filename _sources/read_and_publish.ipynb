{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "def cleanup_page(url):\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    mc = soup.find('div', {'id': 'main-content'}).extract()\n",
    "    for div in mc.find_all(\"a\", {'class':'headerlink'}): \n",
    "        div.decompose()\n",
    "    title = mc.find('h1').text\n",
    "    for tag in mc.find_all(['div', 'span']): \n",
    "        tag.unwrap()\n",
    "    for tag in mc.find_all(\"pre\"): \n",
    "        tag.string = tag.text.replace('%%sql', '')\n",
    "    for tag in mc.find_all([\"script\", 'h1']): \n",
    "        tag.decompose()\n",
    "    mc.find('div')\n",
    "    clean_html_filtered = filter(lambda e: str(e).strip() not in ('', '\\n'), mc.contents)\n",
    "    clean_html_list = [str(e) for e in clean_html_filtered]\n",
    "    clean_html_list = filter(lambda e: str(e).strip() not in (''), str(bs('\\n'.join(clean_html_list), 'html.parser')).splitlines())\n",
    "    return title, str(bs('\\n'.join(clean_html_list), 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_landing_page_url(base_url):\n",
    "\n",
    "    base_page = requests.get(base_url)\n",
    "    base_soup = bs(base_page.content, 'html.parser')\n",
    "\n",
    "    landing_page_url = f\"{base_url}/{base_soup.find('meta')['content'].split('=')[1]}\"\n",
    "    return landing_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_level_urls(landing_page_url):\n",
    "    landing_page = requests.get(landing_page_url)\n",
    "    landing_soup = bs(landing_page.content, 'html.parser')\n",
    "    \n",
    "    first_level_urls = []\n",
    "    for link in landing_soup.find_all('a', {'class': 'reference internal'}):\n",
    "        if link.get('href') and link.get('href') != '#':\n",
    "            first_level_urls.append(link.get('href'))\n",
    "    \n",
    "    return first_level_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(first_level_urls):\n",
    "    all_urls = []\n",
    "    for url in first_level_urls:\n",
    "        all_urls.append(f\"{base_url}/{url}\")\n",
    "        second_level_url = f\"{base_url}/{url}\"\n",
    "        second_level_page = requests.get(second_level_url)\n",
    "        second_level_soup = bs(second_level_page.content, 'html.parser')\n",
    "        current_active = second_level_soup.find('li', {'class': 'toctree-l1 current active'})\n",
    "        for link in current_active.find_all('a', {'class': 'reference internal'}):\n",
    "            all_urls.append(f\"{base_url}/{url.split('/')[0]}/{link.get('href')}\")\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(base_url):\n",
    "    landing_page_url = get_landing_page_url(base_url)\n",
    "    first_level_urls = get_first_level_urls(landing_page_url)\n",
    "    all_urls = get_all_urls(first_level_urls)\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_yt(html):\n",
    "    soup = bs(html, 'html.parser')\n",
    "    for iframe in soup.find_all('iframe'):\n",
    "        if iframe.get('src') and 'youtube.com' in iframe.get('src'):\n",
    "            return iframe.get('src').split('/')[-1].split('?')[0]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://python.itversity.com'\n",
    "\n",
    "all_urls = scrape(base_url)\n",
    "\n",
    "videos = []\n",
    "for url in all_urls:\n",
    "    title, clean_html = cleanup_page(url)\n",
    "    # Capturing Video id to update the metadata (such as description) in YouTube\n",
    "    video_id = check_yt(clean_html)\n",
    "    if video_id:  \n",
    "        videos.append((url, title, video_id, clean_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "YT_DESCRIPTION_TEMPLATE = '''\n",
    "{DESCRIPTION}\n",
    "\n",
    "You can access complete content of Mastering Python by following this Playlist on YouTube - {COURSE_PLAYLIST}\n",
    "\n",
    "You can access complete content related to this video by clicking on {CONTENT_LINK}\n",
    "\n",
    "{SOCIAL_LINKS}\n",
    "\n",
    "{HASH_TAGS}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(html):\n",
    "    soup = bs(html, 'html.parser')\n",
    "    p = soup.find('p')\n",
    "    return p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_PLAYLIST = 'https://www.youtube.com/playlist?list=PLf0swTFhTI8rkH4yIfoyTAheEGjWIRtPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOCIAL_LINKS = '''\n",
    "* Newsletter: http://notifyme.itversity.com\n",
    "* LinkedIn: https://www.linkedin.com/company/itversity/\n",
    "* Facebook: https://www.facebook.com/itversity\n",
    "* Twitter: https://twitter.com/itversity\n",
    "* Instagram: https://www.instagram.com/itversity/\n",
    "* YouTube: https://www.youtube.com/itversityin\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_TAGS = '#MasteringPython #BuildApplications #DataEngineering #ExtremeAutomation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_description = []\n",
    "for video in videos:\n",
    "    description = YT_DESCRIPTION_TEMPLATE.format(\n",
    "        DESCRIPTION=get_description(video[3]),\n",
    "        CONTENT_LINK=video[0],\n",
    "        COURSE_PLAYLIST=PYTHON_PLAYLIST,\n",
    "        SOCIAL_LINKS=SOCIAL_LINKS,\n",
    "        HASH_TAGS=HASH_TAGS\n",
    "    )\n",
    "    videos_with_description.append((video[2], description))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(videos_with_description[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
